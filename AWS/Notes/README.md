# üå•Ô∏è Introduction to Cloud Computing

## Understanding the Cloud

- **What is the Cloud?**
  - A **server** hosted somewhere that clients (web browsers) access via a **network** to visualize websites.
  - Clients and servers communicate using **IP addresses**.
  
- **Analogy**: 
  - Think of sending a letter:
    - **Client**: You (writing the letter)
    - **Network**: Postal system
    - **Server**: Your friend (the recipient)

- **Components of a Server**:
  - **CPU**: Performs computations (like the brain)
  - **RAM**: Fast memory for quick data access
  - **Storage**: For files and structured data (databases)
  - **Networking**: Routers, switches, and DNS servers

- **Challenges with Traditional IT**:
  - Costs: Rent, power, cooling, maintenance.
  - Scaling: Limited by physical space and time to acquire new servers.
  - Disasters: Vulnerability to events like earthquakes or fires.

---

## Cloud Computing Basics ‚òÅÔ∏è

- **Definition of Cloud Computing**:
  - The **on-demand delivery** of compute power, database storage, applications, and other IT resources.
  - **Pay-as-you-go pricing**: Pay only for what you use, when you use it.

- **Benefits of Cloud Computing**:
  - **Instant Access**: Get resources within seconds.
  - **Flexibility**: Scale up or down as needed.
  - **User-Friendly Interface**: Easy access to servers, storage, and applications.

- **Everyday Examples of Cloud Services**:
  - **Gmail**: Email storage service
  - **Dropbox**: Cloud storage service
  - **Netflix**: Video-on-demand service built on AWS

---

## Types of Cloud Computing üîç

1. **Infrastructure as a Service (IaaS)**:
   - **Users Manage**:
     - Applications
     - Data
     - Runtime
     - Middleware
     - Operating System
   - **Provider Manages**:
     - Virtualization
     - Servers
     - Storage
     - Networking
   - **Example**: Amazon EC2, Google Cloud, Azure, Rackspace, Digital Ocean, Linode.

2. **Platform as a Service (PaaS)**:
   - **Users Manage**:
     - Applications
     - Data
   - **Provider Manages**:
     - Runtime
     - Middleware
     - Operating System
     - Networking
   - **Example**: AWS Elastic Beanstalk, Heroku, Google App Engine, Windows Azure.

3. **Software as a Service (SaaS)**:
   - **Users Manage Nothing**; everything is handled by the provider.
   - **Examples**: AWS services like Rekognition, Google Apps (Gmail), Dropbox, Zoom.

---

## Pricing Fundamentals and Responsibilities üí∞üîí

- **AWS Pricing Model**:
  - **Pay-As-You-Go Model**:
    - **Compute**: Pay for actual compute time used.
    - **Storage**: Pay for the exact amount of data stored in the cloud.
    - **Networking**: Pay only when data leaves the cloud (data entering the cloud is free).

- **Security in the Cloud**:
  - **Customer Responsibilities**:
    - Your data
    - Configuration settings
    - Operating system security
    - Network and firewall configurations
    - Overall security measures implemented within your cloud environment

  - **AWS Responsibilities**:
    - Security of the Cloud:
      - The underlying infrastructure, including hardware and software
      - Internal security measures and controls
      - Ensuring the cloud services are secure and reliable

- **Importance of the Shared Responsibility Model**:
  - **Shared Responsibility**: Understanding the split between your responsibilities and those of AWS is crucial for effective cloud security management.
  - **Exam Relevance**: In the Certified Cloud Practitioner exam, expect questions regarding the delineation of responsibilities under the Shared Responsibility Model.

- **Acceptable Use Policy**:
  - **Agreement**: By using AWS, you agree to adhere to their Acceptable Use Policy.
  - **Prohibited Actions**:
    - Engaging in illegal, harmful, or offensive activities
    - Security violations (e.g., hacking, unauthorized access)
    - Network abuse (e.g., DDoS attacks)
    - Email and messaging abuse (e.g., spamming)

---

## History of the AWS Cloud üìú

- **Launch of AWS**:
  - **2002**: AWS was launched internally at Amazon.com, recognizing the potential to externalize their IT departments.
  - **2004**: Public offering of **SQS** (Simple Queue Service).
  - **2006**: Expansion with the introduction of **S3** (Simple Storage Service) and **EC2** (Elastic Compute Cloud). 

- **Global Reach**:
  - AWS has applications used by companies like **Dropbox**, **Netflix**, **Airbnb**, and even **NASA**.
  - AWS has been recognized as a **leader** in the Magic Quadrant from Gartner for many years.

- **Current Statistics** (as of 2023):
  - Revenue of **$90 billion** and accounts for about **31%** of the market in Q1 2024, with Microsoft in second place at **25%**.
  - Over **1 million active users**.

- **Building on AWS**:
  - AWS enables the creation of sophisticated and scalable applications for diverse industries:
    - **Use Cases**:
      - Transferring enterprise IT
      - Backup and storage
      - Big data analytics
      - Hosting websites
      - Mobile and social application backends
      - Gaming servers

- **Global Infrastructure**:
  - **Regions**: Clusters of data centers named (e.g., US-East-1, EU-West-3) that host AWS services.
  - **Availability Zones**: Each region has multiple availability zones (minimum of three), isolated from disasters and connected via high-bandwidth, low-latency networking.
  - **Points of Presence (Edge Locations)**: Over **400 points of presence** across 90 cities in 40 countries to optimize content delivery.

- **Choosing an AWS Region**:
  - **Factors to Consider**:
    - **Compliance**: Data may need to remain local.
    - **Latency**: Proximity to users can reduce lag.
    - **Service Availability**: Not all regions have all AWS services.
    - **Pricing**: Costs may vary between regions.


## Conclusion üéâ

- Cloud computing offers flexibility, cost-effectiveness, scalability, and agility.
- Understanding the Shared Responsibility Model is essential for cloud security.

---

# üõ°Ô∏è IAM (Identity and Access Management)

### Overview
- IAM stands for **Identity and Access Management**.
- It is a **global service** where we create users and assign them to groups.
- When creating an AWS account, a **root account** is automatically created. 
  - This root user should only be used for initial setup and not for regular tasks.
  
### Users and Groups
- Each user represents one person within the organization.
- Users can be grouped for easier management:
  - **Example**: In an organization with six people (Alice, Bob, Charles, David, Edward, Fred):
    - **Developers Group**: Alice, Bob, and Charles.
    - **Operations Group**: David and Edward.
- Groups can only contain users; they cannot contain other groups.
- A user can belong to multiple groups.
- Users not in a group is possible but not a best practice.

### Permissions
- Users or groups are assigned **IAM policies**, which are JSON documents defining allowed actions.
- **Example Policy**:
  - Allows users to:
    - Use the EC2 service and perform describe actions.
    - Use the Elastic Load Balancing service and perform describe actions.
    - Use CloudWatch.
- AWS follows the **principle of least privilege**:
  - Only grant permissions that a user needs.
  - Avoid giving users unrestricted access to prevent high costs or security vulnerabilities.

### IAM Policies
- When a policy is attached to a group (e.g., Developers), all members inherit that policy.
- Users can have **inline policies** attached directly, independent of group memberships.
- **Example**: If Charles and David are in the audit team, they will inherit that policy in addition to their respective group policies.

### Policy Structure
- IAM policy structure consists of:
  - **Version**: Usually `2012-10-17`.
  - **ID**: Optional identifier for the policy.
  - **Statements**: One or more statements.
  
Each statement includes:
- **Sid**: Optional statement identifier.
- **Effect**: Whether the statement allows or denies access (e.g., allow or deny).
- **Principal**: Accounts, users, or roles to which the policy applies.
- **Action**: List of API calls that are allowed or denied.
- **Resource**: List of resources to which the actions apply.
- **Condition**: Optional criteria for when the statement should apply.

---

## Protecting Users and Groups in IAM üîê

### Defense Mechanisms
1. **Password Policy**
   - A strong password increases security for your accounts.
   - In AWS, you can set up a password policy with various options:
     - **Minimum Password Length**: Define the minimum number of characters.
     - **Character Types**: Require specific character types (uppercase, lowercase, numbers, non-alphanumeric).
     - **User Password Change**: Allow users to change their passwords or require them to change after a certain period (e.g., every 90 days).
     - **Prevent Password Reuse**: Users cannot revert to previous passwords when changing.
   - A password policy helps protect against brute force attacks.

2. **Multi-Factor Authentication (MFA)**
   - MFA adds an additional layer of security beyond just a password.
   - Users should enable MFA, especially for accounts with higher privileges, to prevent unauthorized access.
   - MFA combines something you know (password) with something you own (security device).
   - **Example**: Alice knows her password and has an MFA device generating a token for login.

### Benefits of MFA
- If Alice‚Äôs password is stolen, her account remains secure as a hacker would also need her physical MFA device (e.g., a phone) to log in.

### MFA Device Options in AWS
- Familiarize yourself with these options for the exam:
  1. **Virtual MFA Device**: 
     - Use apps like Google Authenticator or Authy.
     - Supports multiple tokens on a single device.
  2. **Universal 2nd Factor (U2F) Security Key**: 
     - A physical device, like a YubiKey by Yubico, allowing multiple users with one key.
  3. **Hardware Key Fob MFA Device**: 
     - Examples include devices from Gemalto, which are third-party providers.
  4. **Government Cloud MFA**: 
     - Special key fob provided by SurePassID for users of AWS GovCloud.

---

## Accessing AWS
There are three main ways to access AWS:

1. **Management Console** (Web Interface)
   - Protected by username, password, and possibly MFA.
   
2. **Command Line Interface (CLI)**  
   - Allows interaction with AWS services through terminal commands.
   - Requires **access keys** (Access Key ID and Secret Access Key) for authentication.
   - Example:
     - `aws s3 cp` to copy files in S3.
   - Access keys are private and should never be shared. Treat your Access Key ID like a username and your Secret Access Key like a password.

3. **Software Development Kit (SDK)**
   - Allows API calls to AWS from within your code.
   - Different SDKs exist for various programming languages (e.g., Python, Java, JavaScript, .NET).
   - Example: The **AWS CLI** itself is built on **Boto**, the AWS SDK for Python.
   
---

### CLI Overview
- The CLI enables direct access to AWS services via public APIs.
- Useful for automating tasks and managing resources through scripting.
- It's open-source and available on GitHub.

### SDK Overview
- The SDK is integrated into your application to manage AWS services programmatically.
- Available for languages like Python, Java, JavaScript, Ruby, and more.
- Specialized SDKs exist for mobile (Android, iOS) and IoT devices.

---

## IAM Roles

IAM Roles allow AWS services to perform actions on your behalf. Just like users, services need permissions, and these are granted through IAM Roles.

#### What are IAM Roles?
- IAM Roles function similarly to users but are designed for AWS services rather than people.
- AWS services (like EC2 instances or Lambda functions) use these roles to get permissions to perform actions on AWS resources.

#### Example: EC2 Instance Roles
- When we launch an **EC2 Instance** (a virtual server), it may need to interact with other AWS services.
- To allow the EC2 instance to perform actions, we assign it an IAM Role.
  - The role gives the EC2 instance the permissions it needs.
  - Example: If the role allows it, the EC2 instance can retrieve information or perform tasks on AWS.
  
#### Common IAM Role Use Cases:
- **EC2 Instance Roles** ‚Äì Allow EC2 instances to access other AWS services.
- **Lambda Function Roles** ‚Äì Provide AWS Lambda functions with the necessary permissions.
- **CloudFormation Roles** ‚Äì Allow CloudFormation to manage AWS resources on your behalf.

---

## IAM Security Tools

IAM offers several security tools to help manage and monitor user credentials and access within your AWS account.

#### 1. IAM Credentials Report
- **Account-level report** that includes all users in your account.
- Provides the status of their credentials (e.g., passwords, access keys).
- Helps you review and monitor credential usage across your AWS account.

#### 2. IAM Access Advisor
- **User-level tool** that shows service permissions granted to a user and when those services were last accessed.
- Useful for applying the **principle of least privilege** by identifying unused permissions and adjusting user access accordingly.

These tools are essential for maintaining security and ensuring users have the appropriate permissions within your AWS account.

---

## Shared Responsibility Model for IAM

The **Shared Responsibility Model** for IAM outlines what AWS is responsible for and what falls under your responsibility when it comes to managing identity and access in the cloud.

#### AWS Responsibilities
- **Infrastructure security:** AWS manages the underlying infrastructure, including physical security and the global network.
- **Service availability:** AWS ensures the availability and security of IAM as a service.
- **Compliance of IAM services:** AWS is responsible for ensuring that IAM services meet industry compliance standards.

#### Customer Responsibilities
- **User, Group, and Role Management:** You are responsible for creating and managing users, groups, roles, and policies in IAM.
- **Policy Management:** Defining, attaching, and managing policies is your responsibility.
- **MFA Enforcement:** You must enable and enforce Multi-Factor Authentication (MFA) on accounts.
- **Access Key Rotation:** Regular rotation of access keys is required to ensure security, and you must manage this.
- **Permissions Management:** Applying the principle of least privilege and ensuring the correct permissions are assigned to users, groups, and roles is your responsibility.
- **Access Monitoring:** Reviewing IAM permissions, auditing access patterns, and revoking unnecessary access is up to you, not AWS.

---

## IAM Best Practices

To ensure proper management and security when using IAM, here are some best practices:

- **Avoid using the root account:** Use your root account only for initial setup. Create individual IAM users for everyday tasks.
- **One user per person:** Ensure that each person using AWS has their own IAM user. Do not share credentials between users.
- **Use groups for permissions:** Assign users to groups and manage permissions at the group level to maintain security and simplify access management.
- **Implement strong password policies:** Create and enforce a strong password policy to secure user access.
- **Enable Multi-Factor Authentication (MFA):** Require MFA for all users, especially for sensitive roles, to protect against unauthorized access.
- **Use IAM roles for AWS services:** When providing access to AWS services, such as EC2 instances, always use IAM roles instead of credentials.
- **Secure access keys:** If you are using AWS programmatically (CLI or SDK), generate access keys and treat them like passwords. Keep them secret and do not share them.
- **Review permissions regularly:** Use the IAM credentials report and IAM Access Advisor to audit permissions and remove unnecessary access.
- **Never share IAM credentials:** Do not share IAM users or access keys. Keep them private to maintain security.


# üíº AWS EC2 (Elastic Compute Cloud)

### What is Amazon EC2?
- **Amazon EC2 (Elastic Compute Cloud)** is a service that allows you to rent virtual servers, known as **EC2 instances**, on demand. It provides scalable compute power in the cloud and is fundamental for understanding AWS and cloud computing.
- EC2 is part of the **Infrastructure as a Service (IaaS)** model, where you can launch and manage virtual machines with flexible configurations.

---

### Key EC2 Components
1. **EC2 Instances**:  
   - Virtual machines that can run various operating systems such as **Linux**, **Windows**, or even **Mac OS**.
   
2. **EBS (Elastic Block Store)**:  
   - Network-attached storage that can be used for persistent data storage.
   
3. **Elastic Load Balancer (ELB)**:  
   - Distributes incoming traffic across multiple instances for high availability and fault tolerance.
   
4. **Auto Scaling Group (ASG)**:  
   - Automatically adjusts the number of EC2 instances based on the current demand.

---

### EC2 Instance Configuration Options
- When launching an EC2 instance, several configuration options are available:
  1. **Operating System (OS)**: Choose between Linux, Windows, or Mac OS.
  2. **Compute Power**: Select the number of **vCPUs** (virtual CPUs) and **RAM** based on the requirements of your workload.
  3. **Storage Options**:
     - **EBS (Elastic Block Store)** or **EFS (Elastic File System)** for network-attached storage.
     - **Instance Store** for storage directly attached to the physical hardware.
  4. **Networking**: Choose the type of **network card**, set up **firewall rules** with **Security Groups**, and assign a **public IP** if needed.
  5. **EC2 User Data**: Run a **bootstrap script** during the instance's first launch to automate tasks like installing updates, software, or downloading files.

---

### Bootstrapping with EC2 User Data
- **Bootstrapping** refers to running commands or scripts when the instance is launched for the first time.
  - This script is called **EC2 User Data** and is only executed once, allowing you to automate tasks like:
    - Installing software and updates.
    - Configuring system settings.
    - Downloading necessary files.
  - The **EC2 User Data** runs as the **root user**, meaning all commands have administrative privileges.

---

### EC2 Instance Examples
- EC2 provides a wide range of instance types to suit various workloads. These instances vary in terms of CPU, memory, network performance, and storage options. Here are a few examples:
  
  1. **t2.micro**:
     - **vCPU**: 1
     - **Memory**: 1 GB RAM
     - **Storage**: EBS only
     - **Network Performance**: Low to moderate
     - **Free Tier Eligibility**: 750 hours per month

  2. **t2.xlarge**:
     - **vCPU**: 4
     - **Memory**: 16 GB RAM
     - **Network Performance**: Moderate

  3. **c5d.4xlarge**:
     - **vCPU**: 16
     - **Memory**: 32 GB RAM
     - **Storage**: 400 GB NVMe SSD
     - **Network Performance**: Up to 10 Gbps

  4. **r5.16xlarge**:
     - **vCPU**: 64
     - **Memory**: 512 GB RAM
     - **Network Performance**: High

---

### EC2 Pricing and AWS Free Tier
- **t2.micro** is part of the **AWS Free Tier**, allowing you to run an instance for **750 hours per month** without charge, making it ideal for learning and experimentation.
- EC2 instances are charged based on the **instance type**, **usage time**, and **data transfer**. 

---

## EC2 Instance Types

AWS provides a variety of EC2 instance types optimized for different use cases. Here's a quick overview:

### General Purpose
- **Use Case**: Suitable for a wide range of workloads, such as web servers and code repositories.
- **Balance**: Compute, memory, and networking.
- **Examples**: `t2.micro` (Free Tier), `m5.large`.

### Compute Optimized
- **Use Case**: Compute-intensive tasks like batch processing, media transcoding, high-performance computing (HPC), and gaming servers.
- **Examples**: `c5.large`, `c6g.xlarge`.

### Memory Optimized
- **Use Case**: Applications needing fast memory access, such as relational and in-memory databases, real-time processing of big data.
- **Examples**: `r5.large`, `x1.32xlarge`.

### Storage Optimized
- **Use Case**: Workloads with high data throughput needs, like OLTP systems, NoSQL databases, and distributed file systems.
- **Examples**: `i3.large`, `h1.8xlarge`.

### Instance Naming Convention
- Example: `m5.2xlarge`
  - **Class**: `m` (General purpose)
  - **Generation**: `5` (5th gen)
  - **Size**: `2xlarge` (Instance size with more memory and vCPUs)

For a full list of EC2 instances, visit [EC2 Instances Info](https://instances.vantage.sh/).

---

## Security Groups and Ports

### Overview
Security Groups act as firewalls for EC2 instances, controlling inbound and outbound traffic. They are a fundamental part of AWS network security.

- **Allow Rules Only**: Security Groups only contain allow rules, specifying what traffic can enter or leave an EC2 instance.
- **IP or Security Group Reference**: Rules can reference IP addresses or other security groups.

### Inbound and Outbound Rules
- **Inbound Traffic**: Controls what is allowed to enter the EC2 instance (e.g., access from your computer).
- **Outbound Traffic**: Controls what traffic the EC2 instance is allowed to send out (e.g., accessing external websites).

### Security Group Features
- **Region/VPC Specific**: Security Groups are tied to specific regions or VPCs. You must create new security groups when changing regions or VPCs.
- **Shared Across Instances**: One Security Group can be applied to multiple instances, and instances can have multiple security groups.
- **Firewall Outside EC2**: Security Groups operate outside the EC2 instance, meaning blocked traffic won't even reach the instance.

### Good Practice
- **Separate SSH Security Group**: It's a good practice to create a separate security group just for SSH access.

### Common Ports
- **SSH (Secure Shell)**: Port 22, used to log into Linux EC2 instances.
- **FTP (File Transfer Protocol)**: Port 21, for uploading files to a file share.
- **SFTP (Secure FTP)**: Port 22, for secure file transfer using SSH.
- **HTTP**: Port 80, for accessing unsecured websites.
- **HTTPS**: Port 443, for accessing secured websites.
- **RDP (Remote Desktop Protocol)**: Port 3389, for logging into Windows instances.

### Advanced Feature: Security Group References
- **Security Group to Security Group Communication**: You can reference one security group in the inbound rules of another. This allows EC2 instances with specific security groups to communicate directly, without needing to manage IP addresses.

#### Example:
Imagine you have two EC2 instances:
1. **EC2 Instance A** with **Security Group A**.
2. **EC2 Instance B** with **Security Group B**.

To allow **Instance B** to communicate with **Instance A**, you would set an inbound rule on **Security Group A** like this:
- **Type**: Custom TCP Rule
- **Protocol**: TCP
- **Port Range**: 8080 (or any port your application uses)
- **Source**: Security Group B

This allows all EC2 instances with **Security Group B** attached to them to communicate with instances that have **Security Group A** attached, through the specified port (8080 in this case). 

You don't need to worry about the IP addresses of the EC2 instances; the security group reference handles it.

---

## EC2 Instance Purchasing Options

AWS offers several purchasing options for EC2 instances based on workload requirements, cost optimization, and usage predictability. Here are the primary purchasing options:

#### 1. **On-Demand Instances**
- **Description**: On-demand instances allow you to pay for compute capacity by the second, with no long-term commitments or upfront payments.
- **Use Case**: Ideal for short-term, unpredictable workloads.
- **Example**: You are running a web server during a product launch that could last a few hours or days, but you don't want long-term commitments.
  
#### 2. **Reserved Instances**
- **Description**: Reserved instances are designed for long-term workloads where you can commit to using the instance for a one- or three-year term, offering significant discounts.
- **Types**:
  - **Standard Reserved Instances**: Up to 72% discount, specific to instance type and region.
  - **Convertible Reserved Instances**: Up to 66% discount, with the flexibility to change instance family or type.
- **Example**: You are running a database that you know will be active for at least three years. You purchase a reserved instance for the long-term and get up to 72% off the on-demand price.

#### 3. **Savings Plans**
- **Description**: Similar to reserved instances but more flexible. You commit to a specific amount of usage (in dollars) over one or three years, regardless of instance type.
- **Use Case**: Long-term, steady-state workloads with flexible instance types and regions.
- **Example**: You commit to spending $300 per month across any EC2 instances in the **m5 family** within **us-east-1** for the next three years.

#### 4. **Spot Instances**
- **Description**: Spot instances are unused EC2 capacity that AWS offers at up to 90% discount, but they can be interrupted with minimal warning.
- **Use Case**: Best suited for fault-tolerant, flexible workloads that can handle interruptions.
- **Example**: You are running image processing tasks that can stop and resume without issue, so you use spot instances to save 90% on costs.

#### 5. **Dedicated Hosts**
- **Description**: This option gives you a physical server fully dedicated to your usage, allowing full control of instance placements and access to the underlying hardware.
- **Use Case**: Required for companies with regulatory or compliance needs or for using your own licensing (e.g., BYOL - Bring Your Own License).
- **Example**: Your organization has strict compliance requirements and needs visibility into the physical server hosting your instances.

#### 6. **Dedicated Instances**
- **Description**: These instances run on hardware dedicated to a single customer, but you don‚Äôt get access to the physical server. It is different from a dedicated host in terms of control and placement.
- **Use Case**: If you need physical isolation for your instances without managing the physical host.
- **Example**: You want hardware that isn‚Äôt shared with other AWS customers, but you don‚Äôt need full control over the host.

#### 7. **Capacity Reservations**
- **Description**: You reserve capacity in a specific availability zone (AZ) for any duration. This guarantees that the capacity is available whenever needed but does not offer any pricing discounts.
- **Use Case**: Suitable for applications that need guaranteed capacity in a particular region but are uncertain about when it will be used.
- **Example**: You are planning a high-traffic event in the near future and want to ensure enough compute capacity is available in a specific AZ during the event.

---

### Summary Table of Purchasing Options:

| **Option**               | **Cost**                    | **Duration**               | **Discount**       | **Best For**                             |
|--------------------------|-----------------------------|----------------------------|--------------------|------------------------------------------|
| **On-Demand**             | Highest                     | None                        | 0%                 | Short-term, unpredictable workloads      |
| **Reserved Instances**    | Medium (Up to 72% discount)  | 1 or 3 years                | Up to 72%          | Long-term, steady-state workloads        |
| **Savings Plans**         | Medium (Up to 72% discount)  | 1 or 3 years                | Up to 72%          | Long-term, flexible usage                |
| **Spot Instances**        | Lowest                      | Variable (can be interrupted)| Up to 90%          | Fault-tolerant, flexible workloads       |
| **Dedicated Hosts**       | Highest                     | On-demand or reserved       | Up to 70% (reserved)| Regulatory/compliance needs              |
| **Dedicated Instances**   | High                        | On-demand or reserved       | No discounts       | Isolated hardware usage                 |
| **Capacity Reservations** | On-demand rates             | Any duration                | No discounts       | Ensuring available capacity              |


---

## Shared responsibility Model for EC2

| **AWS Responsibilities**                                   | **User Responsibilities**                          |
|------------------------------------------------------------|---------------------------------------------------|
| Physical Data Center Security                              | Security Group Rules Configuration                |
| Infrastructure Maintenance (e.g., hardware)                | Operating System Updates & Patches                |
| Physical Host Isolation                                    | Software Installation & Management                |
| Faulty Hardware Replacement                                | IAM Role Assignment & Permissions                 |
| Compliance with Certifications & Regulations               | Data Security (e.g., encryption, access controls)  |

---

# üì¶ EC2 Instance Storage

This section will cover the different storage options for EC2 instances. EC2 storage provides various methods to store and manage data based on use cases. The three main storage types we'll explore are **EBS (Elastic Block Store)**, **EC2 Instance Store**, and **EFS (Elastic File System)**.

## Elastic Block Store (EBS)
EBS Volumes are network drives you can attach to EC2 instances. They persist data even after the instance is terminated, which makes them ideal for long-term storage.

- **Availability Zones**: EBS volumes are bound to specific Availability Zones. For example, an EBS volume created in `us-east-1a` cannot be attached to an EC2 instance in `us-east-1b` unless you create a snapshot and restore it to the new AZ.
- **IOPS (Input/Output Operations per Second)**: When creating an EBS volume, you must provision the capacity and IOPS in advance, depending on your performance needs.
- **Delete on Termination**: This attribute controls whether the EBS volume is deleted when the EC2 instance is terminated. By default, the root volume is deleted, but you can change this behavior.

### Example Use Case:
An EC2 instance in `us-east-1a` has an attached EBS volume. If you stop the instance and start another instance in the same AZ, you can attach the same EBS volume to the new instance and continue where you left off.

### EBS Snapshots:
You can take snapshots (backups) of EBS volumes to create a restore point. Snapshots allow you to copy data across regions or AZs.

- **EBS Snapshot Archive**: Move snapshots to an archive tier for a 75% cost reduction, but note that restoration from the archive takes 24-72 hours.
- **EBS Recycle Bin**: Snapshots can be temporarily stored in a recycle bin before permanent deletion.

---

## EC2 Instance Store
Instance Store is temporary storage attached directly to the physical hardware of your EC2 instance. It offers better I/O performance but does not persist after the instance is stopped or terminated.

- **High Performance**: Useful for caching, scratch data, or temporary content that requires high disk performance.
- **Ephemeral Storage**: Data is lost when the instance stops, so it's not ideal for long-term storage.
- **Backup Responsibility**: You are responsible for backing up and replicating data if needed.

### Example Use Case:
A web application might use EC2 Instance Store to cache frequent reads and writes for faster access. However, the data is not critical, so its loss after termination is acceptable.

---

## Elastic File System (EFS)
EFS is a fully-managed network file system that can be mounted to multiple EC2 instances simultaneously across multiple AZs.

- **Scalability**: Automatically grows and shrinks as you add or remove files.
- **Shared File System**: Ideal for applications needing shared access to files across multiple instances.
- **Cost**: It is more expensive than EBS but charges based on usage, not provisioned capacity.

### Example Use Case:
An application hosted on multiple EC2 instances across `us-east-1a`, `us-east-1b`, and `us-east-1c` can mount the same EFS file system, sharing the same files and making it highly scalable and redundant.

### EFS Storage Classes:
- **EFS Standard**: The default storage class.
- **EFS Infrequent Access (EFS-IA)**: Offers a 92% lower cost for files that are not accessed frequently, with automatic transitions based on lifecycle policies.

---

## Amazon Machine Images (AMIs)

An **Amazon Machine Image (AMI)** represents a pre-configured template that contains the essential information to launch an EC2 instance, including the operating system, software configuration, monitoring tools, and installed applications. AMIs can be either provided by AWS, created by users, or purchased through the AWS Marketplace.

By using an AMI, we can streamline the process of launching new instances, as the required software and configurations are already prepackaged. This leads to faster boot times and configuration, making the process more efficient. AMIs are region-specific but can be copied across regions to leverage the AWS global infrastructure.

AMIs are created from EC2 instances that are first customized to a desired state. Once customized, the instance is stopped to ensure data integrity, and an AMI is created, which also generates associated **EBS snapshots** behind the scenes. These AMIs can then be used to launch other instances with the same configuration in different availability zones or regions.

There are three primary types of AMIs:
- **Public AMIs**: Provided by AWS, like the Amazon Linux 2 AMI.
- **Private AMIs**: Created and maintained by users for internal use.
- **Marketplace AMIs**: Sold by third parties through AWS Marketplace.

Creating AMIs also opens up potential business opportunities, as users can package their software into an AMI and sell it through the AWS Marketplace.

The **EC2 Image Builder** service further automates the process of creating, maintaining, validating, and testing AMIs. It automates the building of EC2 instances (called "Builder EC2 instances") to customize software installations, such as updates to the OS, firewalls, applications, and more. Once the EC2 instance is configured, an AMI is generated automatically.

To ensure quality, **EC2 Image Builder** also validates AMIs by launching test instances to check that the configuration, security, and applications are functioning as expected. If validation passes, the AMI can be distributed across regions, enabling global deployment of applications.

**Key features of EC2 Image Builder**:
- Automates the creation and validation of AMIs.
- Supports scheduled image builds based on criteria like weekly schedules or package updates.
- Free service, only charging for the EC2 instances and storage used during the process.

This seamless automation improves efficiency and helps maintain consistency across deployments.

---

## Shared Responsibility Model for EC2 Storage

| **Component**              | **AWS Responsibility**                                       | **Your Responsibility**                                      |
|----------------------------|--------------------------------------------------------------|--------------------------------------------------------------|
| **EBS Volumes**             | Maintaining the underlying infrastructure                    | Managing data, backups, and snapshots                         |
| **EBS Snapshots**           | Ensuring availability of the snapshot service                | Creating and managing snapshots                               |
| **EC2 Instance Store**      | Managing the physical hardware                               | Managing data, backups, and ensuring replication              |
| **EFS**                     | Maintaining availability and durability of the file system   | Managing data, setting lifecycle policies, and access control |

---

# Elastic Load Balancing and Auto Scaling Groups

## Overview
Elastic Load Balancing (ELB) and Auto Scaling Groups (ASGs) are essential components in building scalable and highly available applications on AWS. They work together to distribute incoming application traffic across multiple targets and automatically adjust the number of running instances based on demand.

---

## Elasticity
**Elasticity** refers to the ability of a cloud environment to dynamically provision and de-provision resources as needed, automatically adjusting to varying workloads. This concept is crucial for maintaining performance while optimizing costs.

### Key Points:
- **Dynamic Resource Management**: Resources can be scaled up or down based on current demand without manual intervention.
- **Cost Efficiency**: Organizations only pay for the resources they actually use, avoiding over-provisioning.
- **Real-time Adjustments**: Elasticity allows systems to react to changes in demand, such as spikes in web traffic during a marketing campaign.

---

## Scalability
**Scalability** is the capability of a system to increase its capacity and accommodate a growing amount of work. This can be achieved by adding resources (vertical scaling) or adding more instances (horizontal scaling).

### Types of Scalability:
1. **Vertical Scalability**: Adding more power (CPU, RAM) to an existing machine.
   - **Example**: Upgrading an EC2 instance type to a larger one to handle increased load.

2. **Horizontal Scalability**: Adding more machines or instances to distribute the load.
   - **Example**: Adding more EC2 instances to an Auto Scaling Group during high traffic periods.

### Key Points:
- **Growth Management**: Scalability ensures that applications can handle increased loads without a drop in performance.
- **Future-proofing**: Scalable architecture can accommodate future growth without significant redesign or restructuring.

---

## High Availability
**High Availability (HA)** refers to the design of systems that ensure a higher operational performance and uptime. This involves minimizing downtime and providing continuous access to services.

### Key Strategies for High Availability:
1. **Redundancy**: Using multiple instances across different Availability Zones to eliminate single points of failure.
   - **Example**: Deploying EC2 instances in multiple AZs so that if one AZ fails, the application remains accessible via instances in another AZ.

2. **Load Balancing**: Distributing traffic among several instances, ensuring that no single instance is overwhelmed.
   - **Example**: Using an Elastic Load Balancer to route requests to healthy EC2 instances, automatically redirecting traffic in case of instance failure.

3. **Health Checks**: Regular monitoring of application components to detect and respond to failures.
   - **Example**: ELB performs health checks on EC2 instances and stops routing traffic to unhealthy instances.

4. **Automatic Recovery**: The system automatically detects and replaces failed components.
   - **Example**: Auto Scaling Groups can terminate unhealthy instances and launch new ones without manual intervention.

---

## Agility
**Agility** in cloud computing refers to the ability to rapidly develop, deploy, and scale applications in response to changing business needs. It allows organizations to innovate faster, adapt to market changes, and improve their overall responsiveness.

### Key Points:
- **Faster Deployment**: Cloud services enable quick provisioning of resources, allowing teams to deploy applications in minutes rather than weeks.
- **Iterative Development**: Agile methodologies, combined with cloud technologies, facilitate continuous integration and delivery (CI/CD), promoting frequent updates and enhancements.
- **Responsive to Change**: Organizations can pivot their strategies quickly, scaling resources up or down based on demand or changing business priorities.
  
### Example:
- A startup can launch a new web application within days using AWS services like EC2. As user demand increases, they can quickly scale their infrastructure with Auto Scaling Groups and Elastic Load Balancers to maintain performance.

---

## Elastic Load Balancing

### What is Elastic Load Balancing?
Elastic Load Balancing is a managed service by AWS that automatically distributes incoming application traffic across multiple EC2 instances, containers, or IP addresses. It provides a single point of access for users, effectively balancing the load to enhance application availability and fault tolerance.

### Key Benefits of ELB
1. **Load Distribution**: Balances incoming traffic across multiple EC2 instances, preventing any single instance from becoming a bottleneck.
2. **Health Checks**: Regularly checks the health of instances and automatically stops routing traffic to unhealthy instances.
3. **SSL Termination**: Simplifies management of SSL/TLS certificates, allowing HTTPS traffic.
4. **High Availability**: Can span multiple Availability Zones (AZs), ensuring the application remains available even if one AZ fails.

### Types of Load Balancers
1. **Application Load Balancer (ALB)**:
   - **Use Case**: Designed for HTTP/HTTPS traffic.
   - **Example**: If you have a web application using microservices architecture, the ALB can route requests based on the URL path, directing traffic to different services.

2. **Network Load Balancer (NLB)**:
   - **Use Case**: Handles TCP and UDP traffic at high performance.
   - **Example**: A gaming application needing low latency can use NLB to manage high volumes of incoming traffic while maintaining performance.

3. **Gateway Load Balancer (GWLB)**:
   - **Use Case**: Routes traffic to virtual appliances, like firewalls or intrusion detection systems.
   - **Example**: If your organization has a firewall running on EC2, the GWLB can direct traffic to it for inspection before reaching the application.


### Architecture Example
- A user accesses a web application through the ALB, which routes the request to one of the available EC2 instances. As more users access the application, the ALB continues to distribute the load effectively.

---

## Auto Scaling Groups

### What is an Auto Scaling Group?
Auto Scaling Groups enable automatic scaling of EC2 instances based on demand, ensuring your application runs optimally at all times without overspending on resources.

### Key Benefits of ASGs
1. **Cost Efficiency**: Only pay for the resources you need, scaling in or out based on demand.
2. **Automatic Recovery**: Detects unhealthy instances and replaces them automatically.
3. **Scalability**: Adjusts the number of EC2 instances up or down based on real-time traffic patterns.

### Components of an ASG
- **Minimum Size**: The minimum number of instances to keep running.
- **Desired Capacity**: The target number of instances, which can change based on scaling policies.
- **Maximum Size**: The maximum number of instances the ASG can scale to.

### Scaling Strategies
1. **Manual Scaling**:
   - Example: Manually changing the capacity of the ASG from 1 instance to 2 during peak hours.

2. **Dynamic Scaling**:
   - **Simple Scaling**:  
     - Example: Set a CloudWatch alarm to add two instances when average CPU utilization exceeds 70% for five minutes.
   - **Step Scaling**:  
     - Example: If CPU usage exceeds certain thresholds, add varying numbers of instances based on the level of usage (e.g., add 1 instance at 70%, 2 instances at 80%).

3. **Target Tracking Scaling**:
   - Example: Set a target to maintain average CPU utilization at 40%. The ASG adjusts the number of instances automatically to stay close to this target.

4. **Scheduled Scaling**:
   - Example: Increase the minimum capacity to 10 instances at 5 PM on Fridays in anticipation of high traffic for a sporting event.

5. **Predictive Scaling**:
   - Example: Using machine learning to analyze historical traffic patterns, the ASG provisions additional instances in advance of expected load peaks, such as during holiday sales.

### Architecture Example
- When user traffic increases, the ASG scales out by launching new EC2 instances. The ELB registers these new instances and starts routing traffic to them, ensuring consistent performance during peak times.

---

## Conclusion
By effectively utilizing Elastic Load Balancing and Auto Scaling Groups, along with understanding concepts of elasticity, scalability, high availability, and agility, AWS allows you to build applications that are not only scalable and highly available but also cost-effective. Implementing these services helps maintain performance and reliability while adapting to changing workloads.

---

# üì¶ Amazon S3 Overview

Amazon Simple Storage Service (S3) is one of the core building blocks of AWS. It is widely used for its **infinitely scalable storage** capabilities, making it a critical part of many web infrastructures.

---

## What is Amazon S3?
Amazon S3 provides secure, durable, and highly scalable object storage. It is used for storing any kind of data, including backups, media, and application data. S3 is an ideal solution for a wide range of use cases, such as:

- **Backup & Storage**: Storing files, data, or disks for backup and retrieval.
  - **Example**: A company might store their daily backups in S3 for safe-keeping.
  
- **Disaster Recovery**: Moving data to another AWS region to ensure availability during outages.
  - **Example**: Data stored in `us-east-1` can be replicated to `eu-west-1` for redundancy.

- **Archival**: Archiving data at a lower cost using S3 Glacier for long-term storage.
  - **Example**: A financial institution might store seven years of transaction history in S3 Glacier for compliance.

- **Hybrid Cloud Storage**: Extending on-premises storage into the cloud using S3.
  - **Example**: A company might store older, infrequently accessed files in S3 to reduce local storage costs.

- **Media Hosting**: Storing and delivering large media files, such as videos or images.
  - **Example**: A video streaming platform might host all its videos on S3.

- **Static Website Hosting**: Hosting a static website directly on S3 without the need for servers.
  - **Example**: A small business could host their informational website using S3.

---

## Key Features of Amazon S3

### Buckets and Objects
Amazon S3 stores data as **objects** within **buckets**. A bucket is a container for storing objects (files), and each object is identified by a **key** (the full path to the file).

- **Bucket**: The top-level directory, globally unique within AWS.
  - **Example**: If you create a bucket named `my-website-data`, no other user in AWS can create a bucket with the same name.

- **Object**: The actual file stored in the bucket, with metadata and versioning options.
  - **Example**: A file stored in the bucket `my-website-data` could have a key `images/photo.jpg`. The full path (`images/photo.jpg`) is the object's key.

### Bucket Naming Rules
- No uppercase letters or underscores.
- Must be between 3 and 63 characters.
- Cannot start with an IP address.
- Must start with a lowercase letter or number.
  
### Example Bucket Names:
- `mycompany-backups`
- `2024-project-files`

### Object Size
- Maximum object size: **5 terabytes**.
- Files larger than **5 GB** must be uploaded in **multiple parts**.
  - **Example**: A large video file (e.g., 10 GB) would need to be split and uploaded using multi-part upload.

### S3 Object Components
1. **Key**: The unique identifier for an object in the bucket.
   - **Example**: The key for a file might be `documents/report.pdf`.
2. **Value**: The content of the file being stored.
3. **Metadata**: Information about the file (e.g., file type, last modified date).
4. **Tags**: Key-value pairs that help manage and filter objects.
   - **Example**: Adding a tag to an object like `Environment:Production` or `Project:2024`.
5. **Version ID**: If versioning is enabled, each object will have a unique version ID.

---

## Amazon S3 Storage Classes

Amazon S3 offers a range of storage classes designed for different use cases, differing in terms of availability, durability, and cost.

### Overview of Storage Classes

1. **S3 Standard (General Purpose)**
2. **S3 Standard-Infrequent Access (S3 Standard-IA)**
3. **S3 One Zone-Infrequent Access (S3 One Zone-IA)**
4. **S3 Glacier Instant Retrieval**
5. **S3 Glacier Flexible Retrieval**
6. **S3 Glacier Deep Archive**
7. **S3 Intelligent-Tiering**

---

### 1. **S3 Standard (General Purpose)**

- **Durability**: 99.999999999% (11 nines)
- **Availability**: 99.99%
- **Use Case**: Frequently accessed data.
- **Features**:
  - Low latency and high throughput.
  - Stores data redundantly across multiple devices in multiple Availability Zones.
- **Examples**:
  - Hosting a frequently accessed website.
  - Storing active content, such as media files for streaming.

### 2. **S3 Standard-Infrequent Access (S3 Standard-IA)**

- **Durability**: 11 nines.
- **Availability**: 99.9%
- **Use Case**: Data accessed less frequently but requires rapid access when needed.
- **Cost**:
  - Lower storage cost than S3 Standard.
  - Retrieval fee applies when accessing data.
- **Examples**:
  - Disaster recovery files.
  - Backup data that needs to be accessible immediately when required.

### 3. **S3 One Zone-Infrequent Access (S3 One Zone-IA)**

- **Durability**: 11 nines within a single Availability Zone.
- **Availability**: 99.5%
- **Use Case**: Data that can be recreated if the Availability Zone is destroyed.
- **Cost**:
  - 20% less than S3 Standard-IA.
- **Examples**:
  - Storing secondary backups.
  - Data that is easily reproducible.

### 4. **S3 Glacier Instant Retrieval**

- **Durability**: 11 nines.
- **Availability**: 99.9%
- **Use Case**: Archive data that needs immediate access.
- **Features**:
  - Millisecond retrieval.
  - Minimum storage duration: 90 days.
- **Examples**:
  - Archived media content that may need to be retrieved instantly.
  - Long-term analytics data requiring quick access.

### 5. **S3 Glacier Flexible Retrieval**

- **Durability**: 11 nines.
- **Availability**: 99.99%
- **Use Case**: Data that is rarely accessed and can tolerate retrieval times from minutes to hours.
- **Retrieval Options**:
  - **Expedited**: 1-5 minutes (higher cost).
  - **Standard**: 3-5 hours.
  - **Bulk**: 5-12 hours (lowest cost).
- **Minimum Storage Duration**: 90 days.
- **Examples**:
  - Compliance and regulatory records.
  - Long-term backups.

### 6. **S3 Glacier Deep Archive**

- **Durability**: 11 nines.
- **Availability**: 99.99%
- **Use Case**: Long-term data archiving with retrieval times in hours.
- **Retrieval Options**:
  - **Standard**: Up to 12 hours.
  - **Bulk**: Up to 48 hours.
- **Minimum Storage Duration**: 180 days.
- **Examples**:
  - Medical records retention.
  - Historical data preservation.

### 7. **S3 Intelligent-Tiering**

- **Durability**: 11 nines.
- **Availability**: 99.9% and 99%
- **Use Case**: Data with unknown or changing access patterns.
- **Features**:
  - Automatically moves data to the most cost-effective access tier.
  - No retrieval fees.
  - Small monthly monitoring and automation fee.
- **Access Tiers**:
  - **Frequent Access Tier**: Default.
  - **Infrequent Access Tier**: Objects not accessed for 30 days.
  - **Archive Instant Access Tier**: Objects not accessed for 90 days.
  - **Archive Access Tier**: Optional, configurable from 90 days.
  - **Deep Archive Access Tier**: Optional, configurable from 180 days.
- **Examples**:
  - Data lakes with unpredictable access patterns.
  - Large datasets where access frequency varies.

---

### Durability and Availability

- **Durability**: The likelihood that your data will not be lost.
  - **11 nines** means that if you store 10 million objects, you can expect to lose one object every 10,000 years.
- **Availability**: The percentage of time that a system is operational and accessible.
  - Higher availability means less expected downtime.

---

## Comparing Storage Classes

| Storage Class                  | Durability       | Availability | Minimum Storage Duration | Retrieval Fees | Use Case                             |
|--------------------------------|------------------|--------------|--------------------------|----------------|--------------------------------------|
| S3 Standard                    | 99.999999999%    | 99.99%       | None                     | No             | Frequently accessed data             |
| S3 Standard-IA                 | 99.999999999%    | 99.9%        | 30 days                  | Yes            | Infrequently accessed, rapid access  |
| S3 One Zone-IA                 | 99.999999999%    | 99.5%        | 30 days                  | Yes            | Infrequently accessed, non-critical  |
| S3 Glacier Instant Retrieval   | 99.999999999%    | 99.9%        | 90 days                  | Yes            | Archive data needing instant access  |
| S3 Glacier Flexible Retrieval  | 99.999999999%    | 99.99%       | 90 days                  | Yes            | Long-term archives, flexible access  |
| S3 Glacier Deep Archive        | 99.999999999%    | 99.99%       | 180 days                 | Yes            | Long-term archives, rare access      |
| S3 Intelligent-Tiering         | 99.999999999%    | 99.9% / 99%  | None                     | No             | Unknown or changing access patterns  |

---

## Selecting the Right Storage Class

- **S3 Standard**: Use when you need low latency and high throughput for frequently accessed data.
- **S3 Standard-IA**: Ideal for data accessed less frequently but requires rapid access when needed.
- **S3 One Zone-IA**: Suitable for infrequently accessed data that can be recreated if lost.
- **S3 Glacier Instant Retrieval**: For archive data that requires immediate access.
- **S3 Glacier Flexible Retrieval**: For archives where retrieval times of hours are acceptable.
- **S3 Glacier Deep Archive**: For long-term retention of data that won't be accessed often.
- **S3 Intelligent-Tiering**: Best when access patterns are unknown or unpredictable.

---

## Managing Storage Classes

### Changing Storage Classes

- You can change the storage class of an object manually via the AWS Management Console, CLI, or SDKs.
- **Example**: Moving an object from S3 Standard to S3 Standard-IA to save on storage costs.

### Lifecycle Policies

- Automate the transition of objects to different storage classes based on predefined rules.
- **Example**: 
  - After 30 days, transition objects from S3 Standard to S3 Standard-IA.
  - After 90 days, move them to S3 Glacier Flexible Retrieval.
  - After 180 days, move them to S3 Glacier Deep Archive.

---

## Best Practices

- **Cost Optimization**: Use the appropriate storage class for your data to balance cost and performance.
- **Data Retrieval Planning**: Be aware of retrieval fees and times for Glacier storage classes.
- **Lifecycle Management**: Implement lifecycle policies to automate data transitions and deletions.
- **Data Protection**: Enable versioning and replication for critical data to prevent accidental loss.
- **Monitoring**: Use AWS monitoring tools to track storage usage and optimize accordingly.

---

## Real-World Examples

- **Media Archive**: A media company stores high-resolution video files in S3 Standard-IA and archives older content in S3 Glacier Deep Archive.
- **Data Lake**: An analytics company uses S3 Intelligent-Tiering for their data lake to handle unpredictable access patterns efficiently.
- **Backup Strategy**: An enterprise uses lifecycle policies to transition backups from S3 Standard to Glacier storage classes over time, optimizing storage costs while retaining data.

---

Amazon S3's variety of storage classes allows you to optimize costs without sacrificing performance, making it a versatile solution for virtually any storage need.

---

## Amazon S3 Encryption

Amazon S3 provides multiple encryption options to secure your data, both **server-side** and **client-side**. Encryption is a critical aspect of protecting sensitive data stored in the cloud.

### 1. **Server-Side Encryption (SSE)**
Server-Side Encryption (SSE) is the most common and is **enabled by default** when you upload objects into an S3 bucket. In this model, Amazon S3 encrypts the data at the server level when it is written to disk, ensuring that the data is protected without requiring any additional steps from the user.

- **How it Works**: 
  - When you upload an object to Amazon S3, it is encrypted automatically before it is stored. The encryption and decryption processes are managed entirely by AWS, transparent to the user.

- **Benefits**:
  - Simplifies encryption management as AWS handles key rotation, encryption, and decryption automatically.
  - Fully integrated with other AWS services.

- **Example**: A user uploads a log file to a bucket, and Amazon S3 automatically encrypts the file.

### 2. **Client-Side Encryption (CSE)**
In **Client-Side Encryption**, the user encrypts the data **before** it is uploaded to S3. This gives the user full control over the encryption process and ensures that only encrypted data is ever sent to AWS.

- **How it Works**: 
  - The user or application encrypts the data locally, then uploads the encrypted object to Amazon S3. The decryption also happens locally upon retrieval of the object.

- **Benefits**:
  - Maximum control over the encryption process and keys.
  - Ideal for use cases where compliance or regulatory requirements dictate that encryption must happen outside the cloud.

- **Drawbacks**:
  - More complexity in managing encryption and decryption, including key storage and access management.

- **Example**: A financial institution might encrypt sensitive customer data on-premises before uploading it to S3 to ensure it never enters AWS unencrypted.

---

## Amazon S3 Shared Responsibility Model

In the **Shared Responsibility Model**, both AWS and the user share responsibilities to ensure the security and management of data stored in Amazon S3. While AWS handles the infrastructure and operational aspects, users are responsible for how they configure and secure their resources.

### Shared Responsibility Model for Amazon S3

| **Responsibility**                          | **AWS Responsibilities**                                                                              | **User Responsibilities**                                                                                             |
|---------------------------------------------|-------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| **Infrastructure Management**               | AWS manages all infrastructure, including the physical storage facilities and the durability/availability of data. | N/A                                                                                                                   |
| **Durability & Availability**               | AWS ensures 11 nines of durability and the ability to sustain failures across multiple facilities.     | N/A                                                                                                                   |
| **Compliance Validation**                   | AWS performs internal vulnerability analysis and compliance validation for their infrastructure.        | N/A                                                                                                                   |
| **S3 Configuration**                        | N/A                                                                                                   | Users must enable features like **S3 Versioning**, **Bucket Policies**, and encryption to secure their data.          |
| **Data Protection**                         | N/A                                                                                                   | Users are responsible for configuring data protection mechanisms, including encryption (SSE, CSE) and versioning.     |
| **Bucket Policy**                           | N/A                                                                                                   | Users must configure **S3 Bucket Policies** to control access to their buckets.                                        |
| **Logging & Monitoring**                    | N/A                                                                                                   | Users must enable **logging** and **monitoring** features, such as **CloudTrail** and **S3 Access Logs**, if needed.  |
| **Cost Optimization**                       | N/A                                                                                                   | Users are responsible for choosing the most cost-effective **S3 storage classes** based on their data access patterns. |
| **Encryption**                              | N/A                                                                                                   | Users must enable **server-side encryption** or perform **client-side encryption** for their data.                    |

---

By understanding the shared responsibilities, you can ensure that your use of Amazon S3 is both secure and cost-effective. AWS handles the core infrastructure and durability, while users must focus on **configuration, security**, and **cost management** of their S3 resources.

---

## AWS Snow Family Overview

The **AWS Snow Family** offers highly secure, portable devices designed to help you collect, process data at the edge, and migrate data to and from AWS. Snow Family devices are perfect when network limitations, bandwidth costs, or unstable connectivity make online transfers difficult or impractical. The family consists of two primary devices: **Snowcone** and **Snowball Edge**.

### Snow Family Devices

1. **Snowcone**:
   - Small, portable device for limited data storage and processing.
   - Storage capacity ranges from **8 to 14 terabytes**.
   - Ideal for small migrations up to a few terabytes.
   - Lightweight and suitable for remote locations or on-the-go usage (e.g., ships, trucks).
   
2. **Snowball Edge**:
   - Larger device with much higher storage and processing capabilities.
   - **Storage capacity** ranges from **80 terabytes to 210 terabytes**.
   - Ideal for large migrations up to petabytes by using multiple Snowball Edge devices.
   - Available in two configurations:
     - **Compute Optimized**: Designed for high-performance computing and edge workloads.
     - **Storage Optimized**: Offers large storage capacity with edge processing.

### Why Use AWS Snow Family?

Snow Family devices provide a solution for offline data migrations and edge computing when transferring data over the network becomes impractical. Here‚Äôs an example:

| **Data Size**      | **Network Speed**          | **Time to Transfer** |
|--------------------|----------------------------|----------------------|
| **100 terabytes**   | 1 Gbps                     | **12 days**          |
| **1 petabyte**      | 10 Gbps                    | **12 days**          |

If transferring large volumes of data would take more than a week, or your bandwidth is limited or unstable, it‚Äôs better to use **Snow Family** devices for offline migration.

### How AWS Snow Family Works

1. **Order**: Request a Snow Family device (Snowcone or Snowball Edge) through the AWS Management Console.
2. **Connect**: Install AWS OpsHub or the Snowball Client on your server and connect to the device.
3. **Transfer Data**: Copy your data to the device using the client tools.
4. **Ship Back**: Send the device back to AWS, and AWS will upload the data to your **Amazon S3 bucket**.
5. **Wipe Device**: AWS will wipe the device clean and prepare it for reuse.

### Edge Computing with AWS Snow Family

Snow Family devices also enable **edge computing**, allowing data to be processed where it is generated, even in locations with limited or no internet connectivity. 

- **Snowcone**: Lightweight with basic processing capability, suitable for small edge locations.
- **Snowball Edge**: Capable of running **EC2 instances** and **AWS Lambda** functions for more advanced edge computing tasks, such as **pre-processing data**, **machine learning**, or **media transcoding**.

### Pricing

Pricing for the AWS Snow Family depends on several factors, including device usage and data transfer. Here's a breakdown:

- **Data Transfer**:
  - **Transferring data into Amazon S3** from a Snow Family device is **free** ($0 per gigabyte).
  - **Transferring data out of AWS** onto your Snow Family device (e.g., Snowball Edge) incurs a cost per gigabyte.
  
- **Device Usage**:
  - **On-demand pricing**: 
    - A **one-time service fee** per job, which includes the first **10 days** of usage for the 80 TB **Snowball Edge Storage Optimized** or **15 days** for the 210 TB version.
    - **Shipping** times are not counted towards the usage days. Shipping from AWS to you and from you back to AWS is included.
    - After the initial 10 or 15 days, you'll pay per additional day of usage.
    
  - **Committed upfront pricing**:
    - You can **prepay** for monthly, one-year, or three-year usage of Snowball Edge devices, particularly for edge computing use cases.
    - This offers up to **62% discounted pricing** for long-term commitments.

From an exam perspective, the key takeaway is that **putting data into Amazon S3 costs $0 per gigabyte**, but any data transfer out of AWS onto the device has associated charges.

### AWS Storage Gateway

AWS Storage Gateway is a hybrid cloud service that connects on-premises environments with AWS storage services. It allows organizations to extend their on-premises storage to the cloud, providing seamless integration between local data centers and AWS for purposes such as:

- **Disaster recovery**
- **Backup and restore**
- **Tiered storage**

#### Types of Storage Gateway:
1. **File Gateway**: Allows you to store files as objects in Amazon S3.
2. **Volume Gateway**: Provides cloud-backed storage volumes, accessible from on-premises applications.
3. **Tape Gateway**: Emulates a physical tape backup infrastructure, storing backups in Amazon S3 and Glacier.

#### Use Cases for Hybrid Cloud:
- **Long cloud migrations**: Useful for gradual data migration to the cloud.
- **Compliance and security**: Meeting specific requirements that demand both on-premises and cloud storage.
- **Storage extension**: Allows on-premises systems to extend storage capabilities into the cloud, leveraging AWS services like S3, EBS, and Glacier.

#### Behind the Scenes:
Storage Gateway uses Amazon EBS, S3, and Glacier as storage backends. It enables organizations to get the best of both worlds: the speed and control of on-premises storage combined with the scalability and cost-effectiveness of cloud storage.

---

# Databases and Analytics

## Amazon RDS
- **Function**: Fully managed relational database service supporting various database engines like MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server.
- **Use Cases**: Ideal for OLTP (Online Transaction Processing) applications.
- **Key Features**:
  - Automated backups, scaling, and replication.
  - Multi-AZ deployments for high availability.
- **Keywords**: RDS, relational database, OLTP, automated backups, high availability.

---

## Amazon Redshift
- **Function**: OLAP (Online Analytical Processing) data warehouse solution for analytics and data warehousing.
- **Use Cases**: Data warehousing and analytics applications; ideal for complex queries and large data sets.
- **Key Features**:
  - Columnar storage for efficient data retrieval.
  - Massively Parallel Processing (MPP) engine for fast query execution.
  - Redshift Serverless option for serverless analytics.
- **Keywords**: Redshift, OLAP, data warehouse, columnar storage, MPP, Redshift Serverless.

---

## Amazon EMR (Elastic MapReduce)
- **Function**: Service for creating and managing Hadoop clusters for big data processing.
- **Use Cases**: Data processing, machine learning, web indexing, and big data analytics.
- **Key Features**:
  - Supports various frameworks like Apache Spark, HBase, and Presto.
  - Auto-scaling and integration with Spot instances for cost efficiency.
- **Keywords**: EMR, Hadoop cluster, big data, data processing, auto-scaling.

---

## Amazon Athena
- **Function**: Serverless query service for analyzing data stored in Amazon S3 using SQL.
- **Use Cases**: Business intelligence, reporting, and log analysis (e.g., VPC flow logs, ELB logs).
- **Key Features**:
  - Supports various file formats (CSV, JSON, ORC, Avro, Parquet).
  - Cost-effective pricing based on data scanned.
- **Keywords**: Athena, serverless, SQL queries, Amazon S3, business intelligence.

---

## Amazon QuickSight
- **Function**: Serverless business intelligence service for creating interactive dashboards and visualizations.
- **Use Cases**: Business analytics, data visualizations, and ad-hoc analysis.
- **Key Features**:
  - Fast, scalable, and embeddable with per-session pricing.
  - Integrates with various data sources like RDS, Redshift, and S3.
- **Keywords**: QuickSight, business intelligence, dashboards, data visualizations.

---

## Amazon DocumentDB
- **Function**: Fully managed NoSQL database service compatible with MongoDB.
- **Use Cases**: Storing, querying, and indexing JSON data.
- **Key Features**:
  - High availability with data replication across three availability zones.
  - Automatically scales storage in 10 GB increments.
- **Keywords**: DocumentDB, NoSQL, MongoDB, JSON data, high availability.

---

## Amazon DynamoDB
- **Function**: Fully managed NoSQL database service for key-value and document data structures.
- **Use Cases**: Applications requiring single-digit millisecond performance at any scale.
- **Key Features**:
  - Automatically scales to accommodate workloads.
  - Supports both key-value and document data models.
  - Global tables for multi-region, fully replicated database.
- **Keywords**: DynamoDB, NoSQL, key-value store, document database, global tables.

---

## Amazon ElastiCache
- **Function**: Fully managed in-memory caching service for improving application performance.
- **Use Cases**: Caching for web applications, session stores, and real-time analytics.
- **Key Features**:
  - Supports Redis and Memcached engines.
  - Automatic failover, backup, and restore capabilities.
  - Seamless scaling and integration with various AWS services.
- **Keywords**: ElastiCache, in-memory caching, Redis, Memcached, application performance.

---

## Amazon Neptune
- **Function**: Fully-managed graph database service for highly connected datasets.
- **Use Cases**: Social networks, knowledge graphs, fraud detection, and recommendation engines.
- **Key Features**:
  - Supports billions of relationships with millisecond latency.
  - Optimized for complex queries on graph data.
- **Keywords**: Neptune, graph database, highly connected datasets, knowledge graphs.

---

## Amazon Timestream
- **Function**: Fully managed time series database service for storing and analyzing time series data.
- **Use Cases**: IoT applications, operational monitoring, and real-time analytics.
- **Key Features**:
  - Automatic scaling for capacity and compute needs.
  - Analyzes trillions of events per day with cost-effective storage.
- **Keywords**: Timestream, time series data, real-time analytics, IoT.

---

## Amazon QLDB (Quantum Ledger Database)
- **Function**: Fully managed, serverless ledger database for recording financial transactions.
- **Use Cases**: Financial applications needing immutable and verifiable transaction records.
- **Key Features**:
  - Immutable system with a cryptographic journal for transaction integrity.
  - High performance compared to traditional ledger systems.
- **Keywords**: QLDB, ledger database, financial transactions, immutable data.

---

## Amazon Managed Blockchain
- **Function**: Service for creating and managing scalable private blockchain networks or joining public ones.
- **Use Cases**: Applications requiring decentralized transactions and collaboration between multiple parties.
- **Key Features**:
  - Supports Hyperledger Fabric and Ethereum frameworks.
  - Decentralized nature for increased security and transparency.
- **Keywords**: Managed Blockchain, decentralized, Hyperledger Fabric, Ethereum, blockchain networks.

---

## AWS Glue
- **Function**: Managed ETL (Extract, Transform, Load) service for data preparation.
- **Use Cases**: Data transformation for analytics and integration across various data sources.
- **Key Features**:
  - Serverless architecture for easier data management.
  - Glue Data Catalog for managing metadata about datasets.
- **Keywords**: AWS Glue, ETL, data preparation, serverless, Glue Data Catalog.

---

## AWS Database Migration Service (DMS)
- **Function**: Service for migrating databases to AWS securely and efficiently.
- **Use Cases**: Homogeneous and heterogeneous database migrations.
- **Key Features**:
  - Source database remains available during migration.
  - Supports various database technologies for seamless migration.
- **Keywords**: DMS, database migration, homogeneous migration, heterogeneous migration, AWS.


---

# üñ•Ô∏è Other Compute Services

## Docker Overview

**Docker** is a software development platform used to package and deploy applications in containers. A container encapsulates everything an application needs to run, making it highly portable across different environments. This eliminates compatibility issues and ensures predictable behavior regardless of where the app is run.

- **Key Benefits:**
  - Works on any operating system and with any programming language.
  - Scales containers up or down quickly, within seconds.
  - Containers can be run on any machine, reducing work and simplifying deployment.
  - Compatible with various technologies such as **Java**, **NodeJS**, **MySQL**, and more.

**Example:**  
On a single EC2 instance, you can run multiple Docker containers, each with different applications such as:
- Docker container running Java code.
- Docker container running a NodeJS application.
- Docker container running a MySQL database.

**Docker Images:**  
- Applications are packaged into **Docker images**, which are stored in Docker repositories.
  - **Public repository:** Docker Hub, where you can find images for technologies like **Ubuntu**, **MySQL**, and **NodeJS**.
  - **Private repository:** AWS provides **Amazon Elastic Container Registry (ECR)** for storing private Docker images.

**Containers vs. Virtual Machines:**  
Docker containers share resources with the host machine, which makes them lightweight compared to virtual machines. Unlike virtual machines, which each run their own guest OS, containers run on the host OS with the help of the **Docker Daemon**.

**Comparing EC2 and Docker:**
- With EC2, each instance has its own OS and applications.
- With Docker, multiple containers can run on one EC2 instance, using fewer resources and providing greater efficiency.

This flexibility makes Docker a powerful tool for modern application deployment, particularly when combined with AWS services like EC2 and ECR.

## Amazon ECS

**Elastic Container Service (ECS)** is used to launch Docker containers on AWS. It requires users to provision and maintain the infrastructure, meaning you need to set up EC2 instances in advance, where AWS will run and manage your Docker containers.

- **Key Points:**
  - Requires EC2 instances to be created in advance.
  - AWS automatically places containers on available EC2 instances.
  - Integration with Application Load Balancer for web applications.
  - Suitable for managing Docker containers directly on EC2.

**Example:**  
You have multiple EC2 instances running various containers, and ECS intelligently assigns containers to the available instances.


## AWS Fargate

**Fargate** is a serverless alternative to ECS, designed to run Docker containers on AWS without the need for provisioning EC2 instances.

- **Key Points:**
  - No need to manage EC2 infrastructure.
  - AWS handles container deployment based on CPU and RAM specifications.
  - Completely serverless and easier to use compared to ECS.

**Example:**  
Fargate automatically runs your Docker container without requiring any EC2 instances, handling everything behind the scenes.



## Amazon ECR

**Elastic Container Registry (ECR)** is a private Docker registry that stores Docker images on AWS. Both ECS and Fargate can pull images from ECR to launch containers.

- **Key Points:**
  - Stores Docker images in a private repository.
  - Used by ECS or Fargate to deploy containers.

**Example:**  
You store your application‚Äôs Docker images in ECR. Fargate then pulls these images and creates containers directly based on your specifications.

## ‚ö° Serverless Services

**Serverless** refers to a computing paradigm where developers focus solely on writing and deploying code without worrying about managing or provisioning servers. The underlying infrastructure is managed by the cloud provider, freeing developers from infrastructure-related tasks.

### Example of Serverless Services

- **Lambda** is a Function as a Service (FaaS) that runs individual functions in response to events, allowing developers to deploy code without server management.  
    - You deploy a function that gets triggered whenever an image is uploaded to **S3**, and Lambda automatically processes that image without the need for provisioning servers.
- **Simple Storage Service (S3)** is a serverless storage service that allows users to store and retrieve data without managing servers, automatically scaling to handle any amount of data.
    - You upload files to **S3**, and S3 handles all the underlying infrastructure without the need for provisioning or managing servers.
- **DynamoDB** is a fully-managed, serverless NoSQL database that automatically scales based on demand, allowing developers to focus on defining tables and data structures.
    - You create a DynamoDB table to store user information for a web application. DynamoDB automatically adjusts its capacity based on the number of users accessing it without you managing any servers.
- **Fargate** is a serverless compute engine for containers, enabling developers to run containers without provisioning or managing EC2 instances.
    - You create a **Docker** container for your web application, and **Fargate** runs it without you having to set up any EC2 instances or infrastructure.



**Serverless** computing enables rapid development by abstracting infrastructure management, allowing developers to focus purely on their code or applications. From Lambda functions to DynamoDB and S3 storage, AWS offers a broad range of serverless services tailored to different use cases.

## AWS Lambda

**Lambda** is AWS's **Function as a Service (FaaS)** offering, enabling users to run functions in the cloud without provisioning or managing servers.

### Key Characteristics:
- **Serverless:** No need to manage underlying infrastructure (no servers to manage or provision).
- **Event-driven:** Lambda functions are triggered by events (e.g., file uploads, API calls).
- **Scaling:** Automatically scales with demand‚Äîno need to set up or manage Auto Scaling groups.

### Key Benefits:
- **Pricing:** Simple and cost-effective, with a generous free tier (1 million invocations and 400,000 GB-seconds of compute time per month).
- **Language Support:** Compatible with multiple languages like Node.js, Python, Java, C#, Ruby, and others through the Custom Runtime API.
- **Integration:** Deep integration with AWS services like S3, DynamoDB, CloudWatch, and EventBridge.
- **Event-driven Scaling:** Lambda automatically handles scaling based on event triggers, making it highly responsive and efficient.

### Lambda Pricing:
- **Requests:** 1 million free requests per month; $0.20 per additional 1 million requests.
- **Duration:** Free tier provides 400,000 GB-seconds of compute time; beyond that, it's $1 per 600,000 GB-seconds.

### Use Cases:
1. **Serverless Thumbnail Creation**
   - Upload an image to **S3** ‚Üí triggers a **Lambda** function ‚Üí Lambda generates a thumbnail ‚Üí stores it back in **S3** and saves metadata in **DynamoDB**.
   - No servers are provisioned for S3, Lambda, or DynamoDB, and the solution scales seamlessly.

2. **Serverless CRON Jobs**
   - Use **CloudWatch Events** or **EventBridge** to trigger a **Lambda** function at scheduled intervals (e.g., hourly, daily) to run tasks like backups or data processing.
   - All serverless, replacing traditional Linux-based CRON jobs with a fully managed, scalable approach.

Lambda is ideal for applications where event-driven, short-duration tasks are needed, and it allows developers to focus solely on writing and deploying code, leaving AWS to handle all server-related concerns.

## Amazon API Gateway

**Amazon API Gateway** is a fully managed service that allows developers to easily create and manage APIs to expose services, particularly in serverless architectures.

### Key Features:
- **Serverless:** Fully managed and serverless, no need to manage underlying infrastructure.
- **RESTful & WebSocket APIs:** Supports both standard **RESTful APIs** (for CRUD operations) and **WebSocket APIs** (for real-time data streaming).
- **Scalable:** Automatically scales to accommodate any number of API requests.
- **Security:** Provides built-in features like **user authentication**, **API throttling**, and **API keys** for managing and securing access to APIs.
- **Monitoring:** Integrated with **CloudWatch** for monitoring API performance and usage metrics.

### Key Use Cases:
- **Serverless HTTP API Creation:** When building serverless architectures with **Lambda** and **DynamoDB**, use API Gateway to expose Lambda functions as APIs that external clients can interact with.
- **Real-Time Data Streaming:** Utilize WebSocket APIs for use cases requiring continuous, real-time data exchange.

### Example Scenario:
1. A **client** sends a request to the **API Gateway**.
2. The API Gateway forwards (proxies) the request to the **Lambda** function.
3. The Lambda function performs operations (e.g., reading or updating data from **DynamoDB**).
4. The response is sent back to the client via the API Gateway.

### Benefits:
- **Simplified API Management:** Easily create, publish, and maintain APIs without worrying about infrastructure.
- **Seamless Integration with Serverless Services:** Works smoothly with Lambda and other AWS services, ideal for building scalable and efficient serverless architectures.

API Gateway is the go-to solution for exposing AWS Lambda functions or other services as HTTP APIs, making it a core component in many serverless applications.

## AWS Batch

**AWS Batch** is a fully managed service that allows for efficient batch processing of jobs at any scale, utilizing the power of AWS infrastructure.

### Key Features:
- **Batch Processing:** Handles batch jobs, which have a clear start and end time, unlike continuous or streaming jobs.
- **Dynamic Scaling:** AWS Batch can dynamically launch **EC2 instances** or **Spot Instances** based on the demand and scale of the batch jobs, optimizing cost and resource usage.
- **Managed Compute:** AWS Batch provisions the right amount of compute and memory automatically based on the requirements of the jobs in the queue.

### Defining a Batch Job:
- **Docker Image + ECS Task Definition:** Batch jobs are defined as **Docker images** with an **ECS task definition**, meaning anything that runs on ECS can run on AWS Batch.

### Example Use Case:
- **Image Processing Pipeline:**
  1. Users upload images to an **S3 bucket**.
  2. This triggers a batch job, which processes the images.
  3. AWS Batch dynamically scales up EC2 or Spot Instances.
  4. The Docker containers run on these instances, performing the image processing.
  5. Processed images are saved back to another S3 bucket.

### AWS Batch vs AWS Lambda:
| **Feature**          | **AWS Batch**                          | **AWS Lambda**                       |
|----------------------|----------------------------------------|--------------------------------------|
| **Time Limit**        | No time limit (runs on EC2 instances)  | 15 minutes                           |
| **Runtime Flexibility**| Any runtime via Docker images         | Limited to specific programming languages (e.g., Python, Node.js) |
| **Disk Space**        | Uses EC2 instance storage (EBS/Instance Store) | Limited temporary disk space         |
| **Scaling**           | Dynamic scaling of EC2/Spot instances  | Automatic, serverless scaling        |
| **Serverless?**       | Managed service, but not serverless    | Fully serverless                     |

AWS Batch is ideal for longer-running, resource-intensive tasks that don't have the constraints of AWS Lambda, offering greater flexibility in runtimes and storage options.

## Amazon Lightsail Notes

**Amazon Lightsail** is a simplified cloud platform designed for users with little to no cloud experience, providing an easy way to get started with virtual servers, storage, databases, and networking. It stands out from other AWS services due to its simplicity and predictable pricing model.

### Key Features:
- **Virtual Servers, Storage, Databases, and Networking:** Everything is bundled into one platform, making it easy for beginners.
- **Low and Predictable Pricing:** Designed for users who need clarity on costs without managing complex cloud services.
- **Templates for Popular Stacks:** Lightsail offers templates for common web applications and environments such as:
  - **LAMP Stack**
  - **Nginx**
  - **MEAN Stack**
  - **Node.js**
  - **WordPress**
  - **Magento**
  - **Plesk**
  - **Joomla**

### Use Cases:
- **Simple Web Applications:** Quick deployment of web apps using pre-configured templates.
- **Development and Testing Environments:** Great for lightweight environments where high availability and AWS service integration are less critical.
  
### High Availability and Limitations:
- **High Availability:** Some built-in high availability features, but no auto-scaling.
- **Limited AWS Integrations:** Lightsail has fewer integrations with the broader AWS ecosystem, making it less flexible than using individual AWS services like EC2, RDS, and others.

### Ideal for:
- **Beginners** with no cloud experience looking for quick setup and low maintenance.
- **Non-technical users** who want to deploy websites or small applications without diving into the complexity of AWS services.

For the **AWS exam**, Lightsail is often the answer when the question refers to users with minimal cloud knowledge needing a simple, low-cost, and quick-to-implement solution.

---